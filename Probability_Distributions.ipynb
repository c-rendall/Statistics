{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains examples of the most popular or common probability distributions often encountered in data science or statistical analysis.  I will neglect any information about background probability theory, which one can easily find in a textbook on probability or statistics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discrete Distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: A *Bernoulli trial* is an experiment having only two outcomes, success (y = 1) or failure (y = 0).  The result *y* of a Bernoulli trial is Bernoulli distributed.    \n",
    "\n",
    "- **Example**: We flip a biased coin, with a probability of $\\theta$ to land on heads.  \n",
    "\n",
    "- **Parameters**: The Bernoulli distribution has a single parameter, $\\theta$, which describes the probability of a success.\n",
    "\n",
    "- **PMF**:         $$ f(y; \\theta) =    \n",
    "\\begin{array}{ll}\n",
    "      1 - \\theta & y = 0\\\\\n",
    "      \\theta & y = 1\\\\\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "- **Usage**: `np.random.choice([0, 1], p=[1-theta, theta])`, `scipy.stats.bernoulli(theta)`\n",
    "\n",
    "- **Related**: The Bernoulli is a special case of the Binomial for N = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: We perform a series of Bernoulli trials with probability of success $\\theta$ until we get a success.  The number of failures *y* before the success is geometrically distributed.  \n",
    "\n",
    "- **Example**: You ask people outside a polling station who they voted for until you find someone that voted for the independent candidate in a local election. The geometric distribution would represent the number of people who you had to poll before you found someone who voted independent.\n",
    "\n",
    "- **Parameters**: The geometric distribution has a single parameter, $theta$, which is the probability that the Bernoulli trial is successful.  \n",
    "\n",
    "- **PMF**: $$ f(y; \\theta) = (1 - \\theta)^{y}\\theta   $$\n",
    "\n",
    "- **Usage**: `np.random.geometric(theta)`, `scipy.stats.geom(theta, loc=-1)`\n",
    "\n",
    "- **Related**: The geometric distribution is a special case of the negative binomial distribution with $\\alpha$ = 1 and $\\theta$ = $\\beta / (1 + \\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: We perform a series of Bernoulli trials.  The number of failures, *y*, before we get $\\alpha$ successses is negative binomially distributed.  This is an extension of the geometric distribution to more than one success.  \n",
    "\n",
    "- **Example**: Suppose we flip a coin repeatedly and count the number of heads (successes). If we continue flipping the coin until it has landed 2 times on heads, we are conducting a negative binomial experiment. \n",
    "\n",
    "- **Parameters**: There are two parameters: $\\alpha$, the desired number of successes, and $\\beta$, the mean of the $\\alpha$ identical Gamma distributions that give the negative binomial.  The probability of success of each Bernoulli trial is given by β/(1+β)\n",
    "\n",
    "- **PMF**: $$ f(y; \\theta)= {y + \\alpha - 1\\choose \\alpha - 1}{\\beta\\choose 1 + \\beta}^{\\alpha}{1\\choose 1 + \\beta}^{y} $$\n",
    "\n",
    "Generally speaking, $\\alpha$ doesn't need to be an integer, so we can write this as: \n",
    "\n",
    "$$ f(y; \\theta)= \\frac{\\Gamma(y + \\alpha)}{\\Gamma(\\alpha)y!}{\\beta\\choose 1 + \\beta}^{\\alpha}{1\\choose 1 + \\beta}^{y} $$\n",
    "\n",
    "- **Usage**: `np.random.negative_binomial(alpha, beta/(1+beta))`, `scipy.stats.nbinom(alpha, beta/(1+beta))`\n",
    "\n",
    "- **Related**: The geometric distribution is a special case of the Negative Binomial distribution in which α=1 and θ=β/(1+β)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: We perform N independent Bernoulli trials, each with probability $\\theta$ of success.  The number of successes *n* is binomially distributed.\n",
    "\n",
    "- **Example**: We sell sandwiches. 70% of people choose chicken, the rest choose something else. We want to know what the probability is of selling 2 chicken sandwiches to the next 3 customers. \n",
    "\n",
    "- **Parameters**: There are two parameters: the probability of success $\\alpha$, and the number of trials *N* \n",
    "\n",
    "- **PMF**: $$ f(n; N, \\theta) = {N\\choose n}(\\theta)^{n}(1 - \\theta)^{N-n}   $$\n",
    "\n",
    "- **Usage**: `np.random.binomial(N, theta)`, `scipy.stats.binom(N, theta)`\n",
    "\n",
    "- **Related**: The Bernoulli distribution is a special case of the Binomial with N = 1.  In the limit of N $\\rightarrow$ $\\infty$ and $\\theta \\rightarrow$ 0, the Binomial distribution becomes a Poisson distribution with parameter N$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: Rare events occur with a rate $\\lambda$ per unit time.  There is no \"memory\" of previous events; the rate is time-independent.  The occurrence of a rare event in this context is referred to as an \"arrival\".  The number of *n* arrivals in unit time is Poisson distributed.  \n",
    "\n",
    "- **Example**: The number of mutations in a strand of DNA per unit length (since mutations are rare) are Poisson distributed.\n",
    "\n",
    "- **Parameter**: The single parameter is $\\lambda$, the rate of rare events occuring.  \n",
    "\n",
    "- **PMF**: $$ f(n;  \\lambda) = \\frac{\\lambda^{n}}{n!}e^{-\\lambda}   $$\n",
    "\n",
    "- **Usage**: `np.random.poisson(lam)`, `scipy.stats.poisson(lam)`\n",
    "\n",
    "- **Related**: In the limit of N→∞ and θ→0 such that the quantity Nθ is fixed, the Binomial distribution becomes a Poisson distribution with parameter Nθ. Thus, for large N and small θ,\n",
    "$$ fPoisson(n;λ)≈fBinomial(n;N,θ) $$ with λ=Nθ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypergeometric Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: Consider an urn with *a* white balls and *b* black balls.  Draw *N* balls from the urn without replacement.  The number of balls of a given color drawn is hypergeometrically distributed. \n",
    "\n",
    "- **Example**: A deck of cards contains 20 cards: 6 red cards and 14 black cards. 5 cards are drawn randomly without replacement. The probability that exactly 4 red cards are drawn is hypergeometrically distributed.\n",
    "\n",
    "- **Parameter**: There are three parameters: the number of draws *N*, the number of white balls *a*, and the number of black balls *b*\n",
    "\n",
    "- **PMF**: $$ f(n; N, a, b)= \\frac{{a \\choose n}{b \\choose N - n}}{{a + b \\choose N}} $$\n",
    "\n",
    "- **Usage**: `np.random.hypergeometric(a, b, N)`, `scipy.stats.hypergeom(a_b, a, N)`\n",
    "\n",
    "- **Related**: In the limit of a+b→∞ such that a/(a+b) is fixed, we get a binomial distribution with parameters N=N and θ=a/(a+b).  The binomial distribution and the hypergeometric are closely related.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: We assign a probability to each of a set of discrete coutcomes,\n",
    "\n",
    "- **Example**:  We have an urn with 10 marbles. Three are red, five blue, and two white.\n",
    "\n",
    "- **Parameter**:  The distribution is parametrized by the probabilities assigned to each event. We define θy to be the probability assigned to outcome y. The set of θy's are the parameters, and have the constraint that they must sum to one.\n",
    "\n",
    "- **PMF**:  $$ f(y; {{[\\theta_{y}]}}) = \\theta_{y}$$\n",
    "\n",
    "- **Usage**:  `np.random.choice(len(theta), p=theta)`, `scipy.stats.rv_discrete(values=(range(len(theta)), theta)).rvs()`\n",
    "\n",
    "- **Related**:  The discrete uniform distribution is a special case when all probabilities are the same.  The Bernoulli distribution is a special case when there are only two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**:  A set of discrete probabilities all with the same value for a given number of outcomes.\n",
    "\n",
    "- **Example**:  We roll a die.  Each of the six sides has equal probability.\n",
    "\n",
    "- **Parameter**:  The distribution is parametrized by the high and low allowed values.\n",
    "\n",
    "- **PMF**:  $$ f(y; y_{low}, y_{high}) = \\frac{1}{y_{high} - y_{low} + 1}$$\n",
    "\n",
    "- **Usage**:  `np.random.randint(low, high+1)`, `scipy.stats.randint(low, high+1)`\n",
    "\n",
    "- **Related**:  The discrete uniform distribution is. a special case of the categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**:  Any outcome in a range has equal probability.\n",
    "\n",
    "- **Example**:  Consider a \"spinner\": an object like an unmagnetized compass needle that can pivots freely around an axis, and is stable pointing in any direction. You give it a spin and see where it comes to rest, measuring the resulting angle (divided by 2𝜋) as a number from 0 to 1.\n",
    "\n",
    "- **Parameter**:  $\\alpha$ and $\\beta$ represent the lower and upper bounds, respectively.\n",
    "\n",
    "- **PDF**:   $$ f(y; \\alpha, \\beta) =    \n",
    "\\begin{array}{ll}\n",
    "      \\frac{1}{\\beta - \\alpha} & \\alpha \\leq y \\leq \\beta\\\\\n",
    "      0 & otherwise \\\\\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "- **Usage**:  `np.random.uniform(alpha, beta)`, `scipy.stats.uniform(alpha, beta)`\n",
    "\n",
    "- **Related**:  The uniform distribution on the interval [0, 1] (i.e., α=0 and β=1) is a special case of the beta distribution where the parameters for the beta distribution are α=β=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: A symmetric, bell-shaped curve.  Generally the most common distribution encountered in statistics or data science. \n",
    "\n",
    "- **Example**: Standardized IQ scores on a test are normally distributed.  In the US, the median IQ is about 100 with a standard deviation of 15 points.  \n",
    "\n",
    "- **Parameter**: The Gaussian distribution has two parameters, the mean μ, which determines the location of its peak, and the standard deviation σ, which is strictly positive (the σ→0 limit defines a Dirac delta function) and determines the width of the peak.\n",
    "\n",
    "- **PDF**: $$ f(y; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-(y-\\mu)^{2}/2\\sigma^{2}}$$\n",
    "\n",
    "- **Usage**: `np.random.normal(mu, sigma)`, `scipy.stats.norm(mu, sigma)`\n",
    "\n",
    "- **Related**: The Gaussian distribution is a limiting distribution in the sense of the central limit theorem, but also in that many distributions have a Gaussian distribution as a limit. This is seen by formally taking limits of, e.g., the Gamma, Student-t, Binomial distributions, which allows direct comparison of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Normal Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: If *ln y* is normally distributed, then *y* is log-normally distributed.\n",
    "\n",
    "- **Example**: The survival time of bacteria in a disinfectant is approximately log-normal. \n",
    "\n",
    "- **Parameter**: Same as the Gaussian, mean $\\mu$ and standard deviation $\\sigma^{2}$\n",
    "\n",
    "- **PDF**: $$ f(y; \\mu, \\sigma) = \\frac{1}{y\\sqrt{2\\pi\\sigma^{2}}}e^{-(ln(y)-\\mu)^{2}/2\\sigma^{2}}$$\n",
    "\n",
    "- **Usage**:  `np.random.lognormal(mu, sigma)`, `scipy.stats.lognorm(x, sigma, loc=0, scale=np.exp(mu))`\n",
    "\n",
    "- **Related**: Be careful not to get confused. The Log-Normal distribution describes the distribution of y given that lny is Gaussian distributed. It does not describe the distribution of lny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: If we have a set of values $X_{1}, ..., X_{n}$ that are normally distributed, then $X^{2}_{1}, ... , X^{2}_{n}$ is $\\chi^{2}$ distributed. \n",
    "\n",
    "- **Example**: The sample variance of ν−1 independent and identically distributed Gaussian random variables, after scaling, is Chi-square distributed. This is the most common use case of the Chi-square distribution.\n",
    "\n",
    "- **Parameter**: There is only one paramter, the degrees of freedom ν\n",
    "\n",
    "- **PDF**: $$ f(y; ν) = \\frac{1}{2^{ν/2}\\Gamma(\\frac{ν}{2})}x^{\\frac{ν}{2}-1}e^{-y/2}$$\n",
    "\n",
    "- **Usage**: `np.random.chisquare(nu)`, `scipy.stats.chi2(nu)`\n",
    "\n",
    "- **Related**: The Chi-square distribution is a special case of the Gamma distribution with α=ν/2 and β=1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student t-Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: The student t-distribution is often used when estimating the mean of a normally-distributed population in situations where the sample size is small and the population's standard deviation is unknown. It is like a Gaussian with fatter tails.  \n",
    "\n",
    "- **Example**: We collect test scores from 20 students on a standardized test.  Since we have a small sample size and an unknown population standard deviation, we can use a student t-distribution to model this. \n",
    "\n",
    "- **Parameter**: The Student-t distribution is peaked, and its peak is located at μ. The peak's width is dictated by parameter σ. Finally, we define the \"degrees of freedom\" as ν. This last parameter imparts the distribution with heavy tails.\n",
    "\n",
    "- **PDF**: $$ f(y;\\mu, \\sigma, ν) = \\frac{\\Gamma(\\frac{ν+1}{2})}{\\Gamma(\\frac{ν}{2})\\sqrt{\\piν\\sigma^{2}}}(1 + \\frac{(y-\\mu)^{2}}{ν\\sigma^{2}})^{\\frac{-ν+1}{2}}$$\n",
    "\n",
    "- **Usage**: `mu + sigma * np.random.standard_t(nu)`, `scipy.stats.t(nu, mu, sigma)`\n",
    "\n",
    "- **Related**: In the n→∞ limit, the Student-t distribution becomes as Gaussian distribution.  The Cauchy distribution is a special case of the Student-t distribution in which ν=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cauchy Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: The Cauchy distribution is a family of distributions which resemble the normal distribution family of curves. While the resemblance is there, it has a taller peak than a normal. And unlike the normal distribution, it’s fat tails decay much more slowly.  The Cauchy distribution is well known for the fact that it’s expected value and other moments do not exist. \n",
    "\n",
    "- **Example**: A Brownian motion in the (𝑥,𝑦)-plane starts at (0,1). The 𝑥-coordinate of the point at which it first reaches the 𝑥-axis has a Cauchy distribution.\n",
    "\n",
    "- **Parameter**: The Cauchy distribution is peaked, and its peak is located at μ. The peak's width is dictated by parameter σ.\n",
    "\n",
    "- **PDF**: $$ f(y;\\mu, \\sigma) = \\frac{1}{\\pi\\sigma}\\frac{1}{1 + (y-\\mu)^{2}/\\sigma^{2}}$$\n",
    "\n",
    "- **Usage**: `mu + sigma + np.random.standard_cauchy()`, `scipy.stats.cauchy(mu, sigma)`\n",
    "\n",
    "- **Related**: The Cauchy distribution is a special case of the Student-t distribution in which the degrees of freedom ν = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: The waiting time for the arrival of a Poisson process is exponentially distributed.  \n",
    "\n",
    "- **Example**: The time between a meteor appearing during a meteor event, a Poisson process, is exponentially distributed.\n",
    "\n",
    "- **Parameter**: The single parameter is the average arrival rate, β. Alternatively, we can use τ=1/r as the parameter, in this case a characteristic arrival time.\n",
    "\n",
    "- **PDF**: $$ f(y;β)=βe^{−βy} $$\n",
    "\n",
    "- **Usage**: `np.random.exponential(1/beta)`, `scipy.stats.expon(loc=0, scale=1/beta)`\n",
    "\n",
    "- **Related**: The exponential distribution is the continuous analog of the geometric distribution. The \"rate\" in the exponential distribution is analogous to the probability of success of the Bernoulli trial. Note also that because they are uncorrelated, the amount of time between any two arrivals is independent of all other inter-arrival times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: The amount of time we have to wait for $\\alpha$ arrivals of a Poisson process is gamma disributed.  If we have events $X_{1}, ..., X_{n}$ that are exponentially distributed, then $X_{1} + ... + X_{n}$ is gamma distributed.  \n",
    "\n",
    "- **Example**: The amount of rainfall accumulated in a reservoir is gamma distributed.\n",
    "\n",
    "- **Parameter**: The number of arrivals $\\alpha$ and the rate of arrivals $\\beta$\n",
    "\n",
    "- **PDF**: $$ f(y;\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}\\frac{(\\beta y)^{\\alpha}}{y}e^{-\\beta y}$$\n",
    "\n",
    "- **Usage**: `np.random.gamma(alpha, beta)`, `scipy.stats.gamma(alpha, loc=0, scale=beta)`\n",
    "\n",
    "- **Related**: The Gamma distribution is the continuous analog of the Negative Binomial distribution. The special case where α=1 is an Exponential distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weibull Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: Distribution of x=yβ if y is exponentially distributed. For β>1, the longer we have waited, the more likely the event is to come, and vice versa for β<1.\n",
    "\n",
    "- **Example**: We want to analyze the lifetime of dental or medical implants.  \n",
    "\n",
    "- **Parameter**: There are two parameters, both strictly positive: the shape parameter β, which dictates the shape of the curve, and the scale parameter τ, which dictates the rate of arrivals of the event.\n",
    "\n",
    "- **PDF**: $$ f(y;\\alpha, \\sigma) = \\frac{\\alpha}{\\sigma}(\\frac{y}{\\sigma})^{\\alpha - 1}e^{-(y / \\sigma)^{\\alpha}}$$\n",
    "\n",
    "- **Usage**: `np.random.weibull(alpha) * sigma`, `scipy.stats.weibull_min(alpha, loc=0, scale=sigma)`\n",
    "\n",
    "- **Related**: The special case where α=0 is the Exponential distribution with parameter β=1/σ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: Conjugate prior of the binomial distribution in Bayesian statistics.  Models a probability of probabilities, as it is bounded on the interval [0,1].\n",
    "\n",
    "- **Example**: Supposing we don't know the exact batting average an MLB player has, we can model the batting average distribution with a beta distribution. \n",
    "\n",
    "- **Parameters**: There are two parameters, both strictly positive: α and β.\n",
    "\n",
    "- **PDF**: $$ f(\\theta;\\alpha, \\beta) = \\frac{\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}$$ where $$ B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}  $$\n",
    "\n",
    "- **Usage**: `np.random.beta(alpha, beta)`, `scipy.stats.beta(alpha, beta)`\n",
    "\n",
    "- **Related**: The Uniform distribution on the interval [0, 1] is a special case of the Beta distribution with α=β=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discrete Multivariate Distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: This is a generalization of the Binomial distribution. Instead of a Bernoulli trial consisting of two outcomes, each trial has K outcomes. The probability of getting $n_{1}$ of outcome 1, $n_{2}$ of outcome 2, ..., and $y_{K}$ of outcome K out of a total of N trials is Multinomially distributed.\n",
    "\n",
    "- **Example**: You roll a die ten times to see what number you roll. There are 6 possibilities (1, 2, 3, 4, 5, 6), so this is a multinomial experiment.\n",
    "\n",
    "- **Parameter**: N, the total number of trials, and $θ=θ_{1},θ_{2},…θ_{k}$, the probabilities of each outcome. Note that $\\Sigma_{i}θ_{i}$=1 and there is a further restriction that $\\Sigma_{i}y_{i}$=N.\n",
    "\n",
    "- **PDF**: $$ f(y;\\theta, N) = \\frac{N!}{y_{1}!, ..., y_{k}!}\\theta^{y_{1}}_{1}, ..., \\theta^{y_{k}}_{k}$$\n",
    "\n",
    "- **Usage**: `np.random.multinomial(N, theta)`, `scipy.stats.nultinomial(N, theta)`\n",
    "\n",
    "- **Related**: The Multinomial distribution generalizes the Binomial distribution to multiple dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continuous Multivariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: This is a generalization of the univariate Gaussian. \n",
    "\n",
    "- **Example**: Finch beaks are measured for beak depth and beak length. The resulting distribution of depths and length is Gaussian distributed.  In this case, the Gaussian is bivariate, with $\\mu = (\\mu_{d}, \\mu_{l})$ and $\\Sigma$ = $\\begin{pmatrix}\n",
    "  \\sigma^{2}_{d} & \\sigma_{dl}\\\\ \n",
    "  \\sigma_{dl} & \\sigma^{2}_{l}\n",
    "\\end{pmatrix}$ \n",
    "\n",
    "- **Parameter**: There is one vector-valued parameter, μ, and a matrix-valued parameter, , referred to respectively as the mean and covariance matrix. The covariance matrix is symmetric and strictly positive definite.\n",
    "\n",
    "- **PDF**: $$ f(y; \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^{K}det\\Sigma}} exp\\Big[\\frac{-1}{2}(y - \\mu)^{T}\\Sigma^{-1}(y - \\mu)\\Big]$$\n",
    "\n",
    "- **Usage**: `np.random.multivariate_normal(mu, Sigma)`, `scipy.stats.multivariate_normal(mu, Sigma)`\n",
    "\n",
    "- **Related**: The Multivariate Gaussian is a generalization of the univariate Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concept**: The Dirichlet distribution is a generalization of the Beta distribution. It is a probability distribution describing probabilities of outcomes. Instead of describing probability of one of two outcomes of a Bernoulli trial, like the Beta distribution does, it describes probability of K−1 of K outcomes. The Beta distribution is the special case of K=2.\n",
    "\n",
    "- **Example**: One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces.\n",
    "\n",
    "- **Parameter**: The parameters are $\\alpha_{1}, .., \\alpha_{K}$, all strictly positive. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
